import sys
import pandas as pd
import numpy
from sklearn.model_selection import train_test_split
from androguard.core.bytecodes.apk import APK

# Modules for Random Forest
# from sklearn.ensemble import RandomForestClassifier
# from sklearn.metrics import accuracy_score

# Modules for DenseNet
from keras.models import Sequential
from keras.layers import Dense

# Whenever we work with machine learning algorithms that use a stochastic process (e.g. random numbers), it is a good idea to set the random number seed.
# This is so that you can run the same code again and again and get the same result. This is useful if you need to demonstrate a result, compare algorithms using the same source of randomness or to debug a part of your code.
numpy.random.seed(7)


applicationLocation = 'application/'
# Get Permissions from the application in the folder
permissions = APK(applicationLocation+"/Swiggy.apk").get_permissions()
# Filter for commonly found permissions to determine viable permissions
permissions = [permission for permission in permissions if "android" in permission]

# Make dataframe from dataset
df = pd.read_csv("final_data.csv")

# Match application features with the dataset features
features = list(df.columns[0:10].values)
APKfeatures = list(set(features).intersection(permissions))
APKValues=[]
for feature in features:
    if feature in APKfeatures:
        APKValues.append(1)
    else:
        APKValues.append(0)

# Set X and Y for the Neural Network Model. Drop unwanted parameters
X = df.drop(['substring','getMemoryInfo','sendTextMessage','getSubscriberId','getLine1Number','loadClass','obtainMessage','getDocumentElement','db','class'],axis=1)
Y = df['class']

#Split in training and testing Datasets
train_X, test_X, train_Y, test_Y = train_test_split(X, Y, test_size=0.25, random_state=0)

# DenseNet
# Models are a sequence of layers. Sequential models add one layer at a time.
model = Sequential()

# Match appropriate input elements, in this case, 10. 
# Function : Dense (number of neurons, input_dim, activation function)
# We will use the rectifier (‘relu‘) activation function on the first two layers and the sigmoid function in the output layer. Better performance is achieved using the rectifier activation function. We use a sigmoid on the output layer to ensure our network output is between 0 and 1 and easy to map to either a probability of class 1

# We can piece it all together by adding each layer. The first layer has 12 neurons and expects 10 input variables. The second hidden layer has 8 neurons and finally, the output layer has 1 neuron to predict the class
model.add(Dense(15, input_dim=10, activation='relu'))
model.add(Dense(10, activation='relu'))
model.add(Dense(1, activation='sigmoid'))

# We will use logarithmic loss, which for a binary classification problem is defined in Keras as “binary_crossentropy“. We will also use the efficient gradient descent algorithm “adam” because it is an efficient default.
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

# The training process will run for a fixed number of iterations through the dataset called epochs, that we must specify using the nepochs argument. We can also set the number of instances that are evaluated before a weight update in the network is performed, called the batch size and set using the batch_size argument.
model.fit(train_X, train_Y, epochs=150, batch_size=10)

# You separate your data into train and test datasets for training and evaluation of your model.
scores = model.evaluate(test_X, test_Y)
print("\n%s: %.2f%%" % (model.metrics_names[1], scores[1]*100))

# We are using a sigmoid activation function on the output layer, so the predictions will be in the range between 0 and 1. We can easily convert them into a crisp binary prediction for this classification task by rounding them.
# Creaing the classifying dataframe and adding the features to the DF
classifyDF = pd.DataFrame(columns=features)
classifyDF = classifyDF.append(pd.Series(APKValues,index=features), ignore_index=True)
predictions = model.predict(classifyDF)
if predictions[0][0] > 0.5:
    print ("This application is Malware.")
else:
    print("This application is Benign.")


#Random Forest
# train_X, test_X, train_y, test_y = train_test_split(X, y, test_size=0.25, random_state=0)
# parameters = {'bootstrap': True,
#               'min_samples_leaf': 3,
#               'n_estimators': 50, 
#               'min_samples_split': 10,
#               'max_features': 'sqrt',
#               'max_depth': 6,
#               'max_leaf_nodes': None}
# RF_model = RandomForestClassifier(**parameters)
# RF_model.fit(train_X, train_y)
# RF_predictions = RF_model.predict(test_X)
# # print(accuracy_score(test_y ,RF_predictions))